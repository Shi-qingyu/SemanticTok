# add the requirement env
sudo apt-get install ffmpeg libsm6 libxext6 tmux htop  -y



cd /mnt/bn/zilongdata-us/xiangtai/SemanticTok/

pip install -r requirements.txt


project=tokenizer_training
batch_size=64
data_path=./data/imagenet/train
model=detok_BB
token_channels=16
patch_size=16
pretrained_model_name_or_path=""
num_register_tokens=4
aux_model_type="dinov2"
aux_dec_type="transformer"
aux_input_type="noisy"
aux_target="align"
reconstruction_weight=1.0
perceptual_weight=1.0
discriminator_weight=0.5
kl_loss_weight=1e-6
aux_loss_weight=1.0

epochs=50
discriminator_start_epoch=1
gamma=3.0
mask_ratio=0.0
mask_ratio_min=0.0
mask_ratio_type="fix"
vit_aux_model_size="tiny"

exp_name="detokBB${pretrained_model_name_or_path}-reg${num_register_tokens}dec-ch${token_channels}-p${patch_size}-g${gamma}-m${mask_ratio_min}${mask_ratio}${mask_ratio_type}-aux${aux_model_type}${aux_dec_type}${aux_input_type}${aux_target}${aux_loss_weight}poolingcls-2025-10-09"

# add variable
export MASTER_ADDR=${ARNOLD_WORKER_0_HOST}
export PORT=(${ARNOLD_WORKER_0_PORT//,/ })
export NPROC_PER_NODE=${ARNOLD_WORKER_GPU}
export NNODES=${ARNOLD_WORKER_NUM}
export NODE_RANK=${ARNOLD_ID}


echo "[INFO] per-GPU batch=${batch_size}"

torchrun \
  --nnodes="${NNODES}" \
  --nproc_per_node="${NPROC_PER_NODE}" \
  --node_rank="${NODE_RANK}" \
  --master_addr="${MASTER_ADDR}" \
  --master_port="${PORT}" \
  main_reconstruction.py \
  --project "${project}" --exp_name "${exp_name}" --auto_resume \
  --batch_size "${batch_size}" --model "${model}" \
  --token_channels "${token_channels}" \
  --patch_size "${patch_size}" \
  --pretrained_model_name_or_path "${pretrained_model_name_or_path}" \
  --num_register_tokens "${num_register_tokens}" \
  --aux_model_type "${aux_model_type}" \
  --aux_dec_type "${aux_dec_type}" \
  --aux_input_type "${aux_input_type}" \
  --aux_target "${aux_target}" \
  --gamma "${gamma}" \
  --mask_ratio "${mask_ratio}" \
  --mask_ratio_min "${mask_ratio_min}" \
  --mask_ratio_type "${mask_ratio_type}" \
  --vit_aux_model_size "${vit_aux_model_size}" \
  --aux_cls_token \
  --pooling_cls_token \
  --reconstruction_weight "${reconstruction_weight}" \
  --perceptual_weight "${perceptual_weight}" \
  --discriminator_weight "${discriminator_weight}" \
  --kl_loss_weight "${kl_loss_weight}" \
  --aux_loss_weight "${aux_loss_weight}" \
  --online_eval \
  --eval_freq 50 \
  --epochs "${epochs}" --discriminator_start_epoch "${discriminator_start_epoch}" \
  --data_path "${data_path}"


tokenizer_project=tokenizer_training
tokenizer_exp_name=${exp_name}
num_register_tokens=${num_register_tokens}

force_one_d_seq=0
exp_name=sit_b-${tokenizer_exp_name}

project=gen_model_training
batch_size=64  # nnodes * ngpus * batch_size = 1024
epochs=100

# add variable
export MASTER_ADDR=${ARNOLD_WORKER_0_HOST}
export PORT=(${ARNOLD_WORKER_0_PORT//,/ })
export NPROC_PER_NODE=${ARNOLD_WORKER_GPU}
export NNODES=${ARNOLD_WORKER_NUM}
export NODE_RANK=${ARNOLD_ID}


echo "[INFO] per-GPU batch=${batch_size}"


torchrun \
    --nnodes="${NNODES}" \
    --nproc_per_node="${NPROC_PER_NODE}" \
    --node_rank="${NODE_RANK}" \
    --master_addr="${MASTER_ADDR}" \
    --master_port="${PORT}" \
    main_diffusion.py \
    --project $project --exp_name $exp_name --auto_resume \
    --batch_size $batch_size --epochs $epochs --use_aligned_schedule \
    --pretrained_model_name_or_path "" \
    --num_register_tokens $num_register_tokens \
    --tokenizer detok_BB --aux_cls_token --pooling_cls_token \
    --use_ema_tokenizer --collect_tokenizer_stats \
    --stats_key $tokenizer_exp_name --stats_cache_path work_dirs/stats.pkl --overwrite_stats \
    --load_tokenizer_from work_dirs/$tokenizer_project/$tokenizer_exp_name/checkpoints/epoch_0049.pth \
    --model SiT_base \
    --force_one_d_seq $force_one_d_seq \
    --num_sampling_steps 250 --cfg 1.3 \
    --cfg_list 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2.0 \
    --online_eval --eval_freq 100 \
    --vis_freq 50 --eval_bsz 256 \
    --data_path ./data/imagenet/train