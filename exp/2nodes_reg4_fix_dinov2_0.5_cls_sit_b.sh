# add the requirement env
sudo apt-get install ffmpeg libsm6 libxext6 tmux htop  -y

export http_proxy=bj-rd-proxy.byted.org:3128  https_proxy=bj-rd-proxy.byted.org:3128  no_proxy=code.byted.org

cd /mnt/bn/zilongdata-us/xiangtai/SemanticTok/

pip install -r requirements.txt


tokenizer_project=tokenizer_training
tokenizer_exp_name=detokBB-reg4-ch16-p16-g3.0-m0.00.0fix-auxdinov2transformernoisyalign0.5cls
num_register_tokens=4

force_one_d_seq=0
exp_name=sit_b-${tokenizer_exp_name}-2025-10-08

project=gen_model_training
batch_size=64  # nnodes * ngpus * batch_size = 1024
epochs=100

# add variable
export MASTER_ADDR=${ARNOLD_WORKER_0_HOST}
export PORT=(${ARNOLD_WORKER_0_PORT//,/ })
export NPROC_PER_NODE=${ARNOLD_WORKER_GPU}
export NNODES=${ARNOLD_WORKER_NUM}
export NODE_RANK=${ARNOLD_ID}


echo "[INFO] per-GPU batch=${batch_size}"


torchrun \
    --nnodes="${NNODES}" \
    --nproc_per_node="${NPROC_PER_NODE}" \
    --node_rank="${NODE_RANK}" \
    --master_addr="${MASTER_ADDR}" \
    --master_port="${PORT}" \
    main_diffusion.py \
    --project $project --exp_name $exp_name --auto_resume \
    --batch_size $batch_size --epochs $epochs --use_aligned_schedule \
    --pretrained_model_name_or_path "" \
    --num_register_tokens $num_register_tokens \
    --tokenizer detok_BB --aux_cls_token \
    --use_ema_tokenizer --collect_tokenizer_stats \
    --stats_key $tokenizer_exp_name --stats_cache_path work_dirs/stats.pkl --overwrite_stats \
    --load_tokenizer_from work_dirs/$tokenizer_project/$tokenizer_exp_name/checkpoints/epoch_0049.pth \
    --model SiT_base \
    --force_one_d_seq $force_one_d_seq \
    --num_sampling_steps 250 --cfg 1.3 \
    --cfg_list 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2.0 \
    --online_eval --eval_freq 100 \
    --vis_freq 50 --eval_bsz 256 \
    --data_path ./data/imagenet/train